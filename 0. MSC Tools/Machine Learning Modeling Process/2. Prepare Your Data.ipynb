{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Prepare Your Data\n",
    "\n",
    "https://machinelearningmastery.com/how-to-prepare-data-for-machine-learning/\n",
    "\n",
    "\n",
    "- Step 1: Select Data\n",
    "- Step 2: Preprocess Data\n",
    "- Step 3: Transform Data\n",
    "\n",
    "\n",
    "## Step 1: Select Data\n",
    "\n",
    "What data is actually needed to address the goal or question. Make assumptions about the data and record these assumptions to make sure your model isnt made on shifting sand. Everything that you assume to be true about your data, whether it be intuition or something else should be treated with a scientific criticism.\n",
    "\n",
    "What data do I need to have?\n",
    "\n",
    "What data do I want have that I don't?\n",
    "\n",
    "What data do I not need?\n",
    "\n",
    "## Step 2: Preprocess Data\n",
    "\n",
    "Formatting - computer files\n",
    "\n",
    "Cleaning - removal or filling of missing data\n",
    "\n",
    "Sampling - If we have a lot of data, break off a piece to work with on your preliminary investigation\n",
    "\n",
    "\n",
    "## Step 3: Transform Data\n",
    "\n",
    "Scaling - Use a standard scaler to reduce the effect of data being exaggerated because it is bigger than other features.\n",
    "\n",
    "Decomposition - Break apart features into component parts if they are more useful than the whole\n",
    "\n",
    "Aggregation - Put together data if it is more useful as a whole\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Identify Outliers\n",
    "\n",
    "\n",
    "https://machinelearningmastery.com/how-to-identify-outliers-in-your-data/\n",
    "\n",
    "\n",
    "\n",
    "Outliers in input data can skew the training process.\n",
    "\n",
    "\n",
    "## Outlier Modeling\n",
    "\n",
    "Some examples of outlier detection methods:\n",
    "\n",
    "- Extreme Value Analysis: Determine the tails of data distributions.\n",
    "- Linear Models: Data that has a high residual from a decomposition regression\n",
    "- Proximity Based Models: Data outside of common clusters\n",
    "\n",
    "\n",
    "\n",
    "## Get Started\n",
    "\n",
    "Pick your poison, but make a stepwise process to check for outliers in the data.\n",
    "\n",
    "Extreme Value Analysis -> clustering methods -> Projection methods\n",
    "\n",
    "### 1. Extreme Value Analysis\n",
    "\n",
    "Visualize the data with scatterplots and histograms and box plots while looking for extreme values. This will only be useful for 1-2 dimensional analysis.\n",
    "\n",
    "Look for values 3 standard deviations from the mean\n",
    "\n",
    "Filter out outliers and assess model performance.\n",
    "\n",
    "\n",
    "### 2. Proximity Methods (Clustering)\n",
    "\n",
    "Identify the natural clusters of the data (K means). Mark the cluster centroids. I dentify data instances that are exceptionally far from cluster centroids. (percentage distance?) \n",
    "\n",
    "Filter out thes outliers and assess model performance\n",
    "\n",
    "\n",
    "### 3. Projection Methods \n",
    "\n",
    "Summarize your data to 2 dimensions with linear decomposition techniques (pca, LDA, SOM?). Then visualize the new mapping and identify outliers by hand.\n",
    "\n",
    "Fiter out and assess\n",
    "\n",
    "\n",
    "## Methods Robust to Outliers\n",
    "\n",
    "Some algorithms are more robust to extreme values in data. (Some may use median rather than mean.)\n",
    "Decision Trees are robust to Outliers, ensemble methods are robust. \n",
    "\n",
    "https://www.quora.com/What-are-methods-to-make-a-predictive-model-more-robust-to-outliers\n",
    "\n",
    "Switch to a more robust error metric. Instead of mean squared error use mean absoluter difference. absoluter deviation instead of standard deviation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Data Preperation\n",
    "\n",
    "- Add attributes - Categorize non numeric features, transform attributes mathematically to take advantage of possible linear relationships, and impute missing data with k nearest neighbor (ie averaging)\n",
    "\n",
    "- Remove Attributes - Project into lower subspace(PCA, LDA), Spatial sign projection (map onto multidim sphere), identify and remove highly correlated attributes\n",
    "\n",
    "- Transform Attributes - Center and scale (standardize), Remove skew, binning\n",
    "\n",
    "\n",
    "# Feature Selection\n",
    "https://machinelearningmastery.com/an-introduction-to-feature-selection/\n",
    "\n",
    "Used to remove irrelevent features that only add noise to your model. Reduces complexity of model, improves performance, blah blah blah\n",
    "\n",
    "## Feature Selection Algorithms:\n",
    "\n",
    "### Filter Methods\n",
    "\n",
    "Assign a statistical score to eah attribute and rank them. Greedy method, not necessarily the best combination of features. Info gain, chi square, correlation coeff scores.\n",
    "\n",
    "### Wrapper Methods\n",
    "\n",
    "Combinations of attributes are tested and compared, search method through different subspaces. best fit, random hill.\n",
    "\n",
    "### Embedded Methods\n",
    "\n",
    "Learn best contributing features during fitting. The algorithm selects features by itself L1 Regularization\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
