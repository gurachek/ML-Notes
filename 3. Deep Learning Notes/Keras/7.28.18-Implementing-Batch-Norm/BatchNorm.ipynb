{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuition and Implementation Batch Normalization\n",
    "\n",
    "In this notebook I will go over the intuition and implementaion of Ioffe and Svegedy's batch normalization.\n",
    "\n",
    "\n",
    "### Batch Normalization\n",
    "\n",
    "- Original paper by Ioffe and Szegedy. <a href=\"https://arxiv.org/abs/1502.03167\"> here. </a>\n",
    "\n",
    "- Insert a batch normalization before or after nonlinearities?\n",
    "<a href=\"http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture6.pdf\"> Usage explanation </a>\n",
    "\n",
    "- For an explanation of the math and implementation in tensorflow. <a href=\"https://towardsdatascience.com/pitfalls-of-batch-norm-in-tensorflow-and-sanity-checks-for-training-networks-e86c207548c8\"> Pitfalls of Batch Norm</a>\n",
    "\n",
    "- Also this post <a href=\"https://towardsdatascience.com/how-to-use-batch-normalization-with-tensorflow-and-tf-keras-to-train-deep-neural-networks-faster-60ba4d054b73\">How to use Batch Normalization with TensorFlow and tf.keras</a>\n",
    "\n",
    "\n",
    "## Notes on Batch Normalization\n",
    "\n",
    "### The Problem\n",
    "\n",
    "If $g(x)$ represents the sigmoid activation function, as $| x |$ increases, $g^\\prime(x)$ tends to zero. For a given hidden layer being trained in a neural network, any unit with an exceptionally large absolute output will create an activation that is large in absolute value. The result is a unit with an exceptionally small gradient, that cannot update its own weights greatly,  nor pass back a significant gradient to train the weights before it.\n",
    "\n",
    "<h6 class=\"text-center\">Sigmoid Function and its Derivative</h6> \n",
    "<img src=\"https://cdn-images-1.medium.com/max/1500/0*K5rfrS3lIXmDh6p6.\" height=300 width=500>\n",
    "\n",
    "\n",
    "So if a large portion of our neurons are outputting exceptionally positive or negative values, then the large area that those neurons are connected to will receive little or no gradient propagation and thus fail to train. This is the vanishing gradient problem, and is largely solved by using a ReLU activation with small learning rates. However, batch normalization is actually an effective method to deal with the tendency of saturable activation functions to create vanishing gradients.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### How it Works\n",
    "\n",
    "If we take a unit in a layer with a sigmoid activation function we would like to see it produce an output to the linear transformation that is close to zero. This way we can ensure that the unit will receive a substantial gradient that will update its own weights as well as the weights of the units in previous layers as back propagation continues.\n",
    "\n",
    "<img src=\"img/sweet.png\">\n",
    "\n",
    "\n",
    "\n",
    "Unfortunately, the linear activations of any unit can very wildly as a result of changes in previous layers caused by wight updates. Batch normalization seeks to address this instability and tendency towards a vanishing gradient by applying preprocessing of inputs at the layer-level. Specifically batch normilation seeks to maintain a unit gaussian distribution of each layer's activation inputs despite changes of the network and variations in data from batch to batch.\n",
    "\n",
    "<h6 class=\"text-center\">Unit Gaussian Distribution</h6> \n",
    "<img src=\"https://www.statlect.com/images/normal_distribution.png\" height=300 width=500>\n",
    "\n",
    "\n",
    "### The Transformation\n",
    "\n",
    "Like normalization in regular preprocessing, with batch normalization we seek a zero-centered, unit variance distribution of batch inputs to our activation functions. During training time we take an activation input $x$ and subtract it by the batch mean $\\mu_{B}$\n",
    "to achieve a zero centered distribution.\n",
    "\n",
    "<br>\n",
    "<div class=\"text-center\">\n",
    "$\\hat{x} = x_i-\\mu_{B}$\n",
    " </div>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "Next we take $x$ and divide it by the batch variance and a small number to prevent division by zero $\\sigma_{B}+\\epsilon$. This ensures that all activation inputs reside in the \"sweet spot\" of the sigmoid activation, where all inputs are close to zero.\n",
    "\n",
    "<br>\n",
    "<div class=\"text-center\">\n",
    "$\\hat{x} = \\frac{x_i-\\mu_{B}}{\\sigma_{B}+\\epsilon}$\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "Lastly we put $\\hat{x}$ through a linear transformation to scale and shift the output of batch normalization $y_i$. Ensuring that this normalizing effect is maintained despite the changes in the network during back propagation. \n",
    "\n",
    "<br>\n",
    "<div class=\"text-center\">\n",
    "$y_i=\\lambda *\\hat{x_i}+\\beta$\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "All of this results in a constant flow of batch activation inputs that do not vary into the saturable regions of the activation function.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"img/sig4.png\">\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "During test time, we do not use batch mean or variance, instead as we train we calculate a moving average and variance to estimate these values for the training population. This simply means collecting the batch means and variances and averaging them together to form an estimate of population mean and variance. These population estimates are used in the batch normalization layers when we propagate a test-sample through the network.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Benefits of Batch Normalization\n",
    "\n",
    "\n",
    "The benefits of batch normalization are the following.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 1. Helps prevent vanishing gradient in networks with saturable nonlinearities (sigmoid, tanh, etc)\n",
    "\n",
    "\n",
    "\n",
    "With Batch normalization we ensure that the inputs of any activation function do not vary into saturable regions by transforming the distribution of those inputs to be unit gaussian (zero-centered and unit variance). This should speed up training time. (as gradients are being passed back without vanishing.)\n",
    "\n",
    "#### 2. Regularizes the model\n",
    "\n",
    "Maybe. Ioffe and Svegeddy make this claim but dont really back it up. Its possible that we are normalizaing the network by constricting the scale of individual neuron outputs, but I need to read further to see if this actually has a normalizing effect.\n",
    "\n",
    "\n",
    "\n",
    "#### 3.  Allows for Higher Learning Rates\n",
    "\n",
    "By preventing issues with vanishing gradient during training, we can afford to set higher learning rates.\n",
    "\n",
    "Batch normalization also reduces dependence on parameter scale. Large learning rates can increase the scale of layer parameters which cause the gradients to amplify as they are passed back during back propagation. I need to read more about this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Implementation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    } else {\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=true;\n",
    "    $('div.input').show()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Hide Code\"></form>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense\n",
    "from keras.layers import MaxPooling2D, Dropout, Flatten\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load and Preprocessing\n",
    "\n",
    "\n",
    "In this notebook we use the Cifar 100 toy dataset, as it is reasonably challenging, and won't take forever to train. The only preprocessing performed is a zero-centering, and a image variation generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar100\n",
    "from keras.utils import np_utils\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n",
    "\n",
    "#scale and regularize the dataset\n",
    "x_train = (x_train-np.mean(x_train))\n",
    "x_test = (x_test - x_test.mean())\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#onehot encode the target classes\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "train_datagen.fit(x_train)\n",
    "\n",
    "train_generator = train_datagen.flow(x_train,\n",
    "                                     y = y_train,\n",
    "                                    batch_size=80,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Model in Keras\n",
    "\n",
    "Convolutional blocks composed of 2 stacked 3x3 convolutions followed by a max pool and dropout. There are 5 convolutional blocks in each network. The final layer is a fully connected layer with 100 nodes and softmax activation. \n",
    "\n",
    "We will be constructing 4 different networks, each with either sigmoid or ReLU activations and either with batch normalization or without. We will compare the time to convergence and validation loss of each of the networks to its colleagues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_block_first(model, bn=True, activation=\"sigmoid\"):\n",
    "    \"\"\"\n",
    "    The first convolutional block in each architecture. Only seperate so we can \n",
    "    specify the input shape.\n",
    "    \"\"\"\n",
    "    model.add(Conv2D(60,3, padding = \"same\", input_shape = x_train.shape[1:]))\n",
    "    if bn:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    \n",
    "    model.add(Conv2D(60,3, padding = \"same\"))\n",
    "    if bn:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    \n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.15))\n",
    "    return model\n",
    "\n",
    "def conv_block(model, bn=True, activation = \"sigmoid\"):\n",
    "    \"\"\"\n",
    "    Generic convolutional block with 2 stacked 3x3 convolutions, max pooling, dropout, \n",
    "    and an optional Batch Normalization.\n",
    "    \"\"\"\n",
    "    model.add(Conv2D(60,3, padding = \"same\"))\n",
    "    if bn:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    \n",
    "    model.add(Conv2D(60,3, padding = \"same\"))\n",
    "    if bn:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    \n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.15))\n",
    "    return model\n",
    "\n",
    "def conv_block_final(model, bn=True, activation = \"sigmoid\"):\n",
    "    \"\"\"\n",
    "    I bumped up the number of filters in the final block. I made this seperate so that\n",
    "    I might be able to integrate Global Average Pooling later on. \n",
    "    \"\"\"\n",
    "    model.add(Conv2D(100,3, padding = \"same\"))\n",
    "    if bn:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    \n",
    "    model.add(Conv2D(100,3, padding = \"same\"))\n",
    "    if bn:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    return model\n",
    "\n",
    "def fn_block(model):\n",
    "    \"\"\"\n",
    "    I'm not going for a very deep fully connected block, mainly so I can save on memory.\n",
    "    \"\"\"\n",
    "    model.add(Dense(100, activation = \"softmax\"))\n",
    "    return model\n",
    "\n",
    "def build_model(blocks=3, bn=True, activation = \"sigmoid\"):\n",
    "    \"\"\"\n",
    "    Builds a sequential network based on the specified parameters.\n",
    "    \n",
    "    blocks: number of convolutional blocks in the network, must be greater than 2.\n",
    "    bn: whether to include batch normalization or not.\n",
    "    activation: activation function to use throughout the network.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    model = conv_block_first(model, bn=bn, activation=activation)\n",
    "    \n",
    "    for block in range(1,blocks-1):\n",
    "        model = conv_block(model, bn=bn, activation = activation)\n",
    "        \n",
    "    model = conv_block_final(model, bn=bn, activation=activation)\n",
    "    model = fn_block(model)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def compile_model(model, optimizer = \"rmsprop\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"]): \n",
    "    \"\"\"\n",
    "    Compiles a neural network.\n",
    "    \n",
    "    model: the network to be compiled.\n",
    "    optimizer: the optimizer to use.\n",
    "    loss: the loss to use.\n",
    "    metrics: a list of keras metrics.\n",
    "    \"\"\"\n",
    "    model.compile(optimizer = optimizer,\n",
    "                 loss = loss,\n",
    "                 metrics = metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "build_model() got an unexpected keyword argument 'classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-05b5a90afda4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msigmoid_without_bn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"sigmoid\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msigmoid_without_bn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompile_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigmoid_without_bn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msigmoid_with_bn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"sigmoid\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msigmoid_with_bn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompile_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigmoid_with_bn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: build_model() got an unexpected keyword argument 'classes'"
     ]
    }
   ],
   "source": [
    "sigmoid_without_bn = build_model(blocks = 5, classes = 100, bn=False, activation = \"sigmoid\")\n",
    "sigmoid_without_bn = compile_model(sigmoid_without_bn)\n",
    "\n",
    "sigmoid_with_bn = build_model(blocks = 5, classes = 100, bn=True, activation = \"sigmoid\")\n",
    "sigmoid_with_bn = compile_model(sigmoid_with_bn)\n",
    "\n",
    "\n",
    "relu_without_bn = build_model(blocks = 5, classes = 100, bn=False, activation = \"relu\")\n",
    "relu_without_bn = compile_model(relu_without_bn)\n",
    "\n",
    "relu_with_bn = build_model(blocks = 5, classes = 100, bn=True, activation = \"relu\")\n",
    "relu_with_bn = compile_model(relu_with_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "\n",
    "\n",
    "### Sigmoid without Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model_checkpoint = ModelCheckpoint('models/sigmoid_without_bn.h5',\n",
    "                                   save_best_only = True)\n",
    "\n",
    "history1 = sigmoid_without_bn.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000,\n",
    "        epochs=20,\n",
    "        verbose=0,\n",
    "        validation_data=(x_test, y_test),\n",
    "        callbacks = [model_checkpoint])\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Training time: \", (end - start)/60, \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "# summarize history for accuracy\n",
    "plt.subplot(121)\n",
    "plt.plot(history1.history['acc'])\n",
    "plt.plot(history1.history['val_acc'])\n",
    "plt.title('Sig w/o BN Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "# summarize history for loss\n",
    "plt.subplot(122)\n",
    "plt.plot(history1.history['loss'])\n",
    "plt.plot(history1.history['val_loss'])\n",
    "plt.title('Sig w/o BN Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid with Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('models/sigmoid_with_bn.h5',\n",
    "                                   save_best_only = True)\n",
    "\n",
    "history2 = sigmoid_with_bn.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000,\n",
    "        verbose=0,\n",
    "        epochs=20,\n",
    "        validation_data=(x_test, y_test),\n",
    "        callbacks = [model_checkpoint])\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Training time: \", (end - start)/60, \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "# summarize history for accuracy\n",
    "plt.subplot(121)\n",
    "plt.plot(history2.history['acc'])\n",
    "plt.plot(history2.history['val_acc'])\n",
    "plt.title('Sig w/ BN Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "# summarize history for loss\n",
    "plt.subplot(122)\n",
    "plt.plot(history2.history['loss'])\n",
    "plt.plot(history2.history['val_loss'])\n",
    "plt.title('Sig w/ BN Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Without Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('models/ReLU_without_BN.h5',\n",
    "                                   save_best_only = True)\n",
    "\n",
    "history3 = relu_without_bn.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000,\n",
    "        epochs=20,\n",
    "        verbose=0,\n",
    "        validation_data=(x_test, y_test),\n",
    "        callbacks = [model_checkpoint])\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Training time: \", (end - start)/60, \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "# summarize history for accuracy\n",
    "plt.subplot(121)\n",
    "plt.plot(history3.history['acc'])\n",
    "plt.plot(history3.history['val_acc'])\n",
    "plt.title('ReLU w/o BN Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "# summarize history for loss\n",
    "plt.subplot(122)\n",
    "plt.plot(history3.history['loss'])\n",
    "plt.plot(history3.history['val_loss'])\n",
    "plt.title('ReLU w/o BN Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU with Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('models/ReLU_with_bn.h5',\n",
    "                                   save_best_only = True)\n",
    "\n",
    "history4 = relu_with_bn.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000,\n",
    "        verbose=0,\n",
    "        epochs=20,\n",
    "        validation_data=(x_test, y_test),\n",
    "        callbacks = [model_checkpoint])\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Training time: \", (end - start)/60, \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "# summarize history for accuracy\n",
    "plt.subplot(121)\n",
    "plt.plot(history4.history['acc'])\n",
    "plt.plot(history4.history['val_acc'])\n",
    "plt.title('ReLU w/ BN Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "# summarize history for loss\n",
    "plt.subplot(122)\n",
    "plt.plot(history4.history['loss'])\n",
    "plt.plot(history4.history['val_loss'])\n",
    "plt.title('ReLU w/ BN Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Training Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "# summarize history for accuracy\n",
    "plt.subplot(121)\n",
    "plt.plot(history1.history['val_acc'])\n",
    "plt.plot(history2.history['val_acc'])\n",
    "plt.plot(history3.history['val_acc'])\n",
    "plt.plot(history4.history['val_acc'])\n",
    "plt.title('All Models Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Sig w/o BN', 'Sig w/ BN', 'ReLU w/o BN', 'ReLU w/ BN'], loc='upper left')\n",
    "# summarize history for loss\n",
    "plt.subplot(122)\n",
    "plt.plot(history1.history['val_loss'])\n",
    "plt.plot(history2.history['val_loss'])\n",
    "plt.plot(history3.history['val_loss'])\n",
    "plt.plot(history4.history['val_loss'])\n",
    "plt.title('ReLU w/ BN loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Sig w/o BN', 'Sig w/ BN', 'ReLU w/o BN', 'ReLU w/ BN'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "Batch normalization is a very useful tool to reduce training time and boost stability of a neural network. These effects are not limited to saturable neural networks with saturable activation functions, they benefit networks with ReLU activation functions as well. In my next notebook I will be summarizing the effects of global average pooling to the training of convolutional neural networks. We will include what we have learned here as we go forward to improve our performance on the Cifar100 dataset.\n",
    "\n",
    "### Further reading\n",
    "\n",
    "Below are some more recent research papers that extend Ioffe and Svegedy's work.\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/1702.03275v2\">[1]</a> How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/1702.03275v2\">[2]</a> Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models \n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/1607.06450v1\">[3]</a> Layer Normalization\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/1602.07868v3\">[4]</a> Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/1803.08494v3\">[5]</a> Group Normalization\n",
    "\n",
    "\n",
    "## Ideas for Future Notebooks:\n",
    "\n",
    "\n",
    "\n",
    "Architecture Improvements\n",
    "- Implement all convolutional network with global average pooling.\n",
    "\n",
    "Parameter Tuning\n",
    "- Experiment with learning rates and loss functions. \n",
    "\n",
    "Applications\n",
    "- How do we predict with a .h5 file and serve with a REST api? \n",
    "- TensorFlow implementation.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
