{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apple Twitter Sentiment Analysis\n",
    "\n",
    "\n",
    "## 0.1 Intent\n",
    "\n",
    "In the following notebook we are going to be performing sentiment analysis on a collection of tweets about Apple Inc. The data can be obtained from the following link. After preprocessing, the tweets are labeled as either positive (i.e. I love my new iphone!) or negative. (i.e. Apple's customer service stinks!) We will conclude this write-up with an analysis of the words that are most indicative of a negative or positive tweet.\n",
    "\n",
    "https://data.world/crowdflower/apple-twitter-sentiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Data Description\n",
    "\n",
    "####  Tweet Sentiment\n",
    "- Positive Sentiment = 1\n",
    "- Negative Sentiment = 0\n",
    "\n",
    "#### Sentiment: Confidence\n",
    "- Range of [0,1] describing the confidence of the assignment.\n",
    "\n",
    "#### Text\n",
    "- Text composition of the tweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Import the Dataset and Preview\n",
    "The raw data comes with a lot of superfluous information that we do not need for this analysis. In this section we toss out these sections and end up with the DataFrame structure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\Harrison\\\\Desktop\\\\APT'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-fcd99295348f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\\\Harrison\\\\Desktop\\\\APT'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin-1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#dropping NaN and \"dont care\" response to setup a binary analysis.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\Harrison\\\\Desktop\\\\APT'"
     ]
    }
   ],
   "source": [
    "os.chdir('C:\\\\Users\\\\Harrison\\\\Desktop\\\\APT')\n",
    "df = pd.read_csv('train.csv', encoding='latin-1')\n",
    "\n",
    "\n",
    "#dropping NaN and \"dont care\" response to setup a binary analysis.\n",
    "df = df[df['sentiment']!=\"3\"]\n",
    "df = df[df['sentiment']!='not_relevant']\n",
    "df = df.reset_index()\n",
    "\n",
    "#dropping the extra columns\n",
    "df = df.drop(['index', '_unit_id', '_golden', '_unit_state', '_trusted_judgments', '_last_judgment_at', 'date','id','query', 'sentiment_gold'], axis =1)\n",
    "\n",
    "def encoder(x):\n",
    "    #A simple class encoder to get to 1s and 0s for each of the sentiment\n",
    "    if x=='5':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#Apply encoder to sentiment column\n",
    "df['sentiment'] = df['sentiment'].apply(encoder)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also see that there is a class imbalance. We have 3 times as many negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\n\\nSentiment Values: ')\n",
    "print(df.sentiment.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing\n",
    "\n",
    "## 1.1 Clean the Data\n",
    "Because the data is made up of Tweets, there are many non-text symbols throughout our dataset. This will only serve to confuse our model and make sentiment predictions less accurate. So we choose to make a simpler and potentially more accurate model by removing those symbols.\n",
    "\n",
    "#### Symbols to be removed.\n",
    "- url symbols (http:\\)\n",
    "- Twitter symbols (@, #)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def standardize_text(df, text_field):\n",
    "    #removes special strings and non text\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    return df\n",
    "\n",
    "df = standardize_text(df, 'text')\n",
    "df.to_csv('clean_apple.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1.2 Data Tokenization\n",
    "Here we turn our twitter strings to lists of individual tokens. (words, punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "#NLTK tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df['tokens'] = df['text'].apply(tokenizer.tokenize)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Sentence Length and Vocabulary Size\n",
    "We have an average sentence length around 14-15 words, and a vocabulary size of 3,711 unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lengths = [len(tokens) for tokens in df['tokens']]\n",
    "vocab = sorted(list(set([word for tokens in df['tokens'] for word in tokens])))\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.xlabel('Sentence Length (in words)')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.title('Sentence Lengths')\n",
    "plt.hist(sentence_lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Words in vocab: ', len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Bag of Words\n",
    "We will use TFIDF to convert our token lists to numerical data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X = df['text']\n",
    "y = df['sentiment']\n",
    "\n",
    "#splitting data for cross validation of model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y)\n",
    "\n",
    "#vectorization with TFIDF and scikit learn\n",
    "vect = TfidfVectorizer()\n",
    "X_train_tfidf = vect.fit_transform(X_train)\n",
    "X_test_tfidf = vect.transform(X_test)\n",
    "\n",
    "#sparse matrix rows << columns\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Visualizing our Dataset\n",
    "We use a singular value decomposition to reduce our dataset to two dimensions. This will allow us to see any inherent linear seperability in the data that our model could capitalize on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def plot_lsa(text, labels, plot=True):\n",
    "    lsa = TruncatedSVD(n_components=2)\n",
    "    lsa.fit(text)\n",
    "    lsa_scores = lsa.transform(text)\n",
    "    color_dict = {label:idx for idx, label in enumerate(set(labels))}\n",
    "    color_column = [color_dict[label] for label in labels]\n",
    "    colors = ['orange', 'blue']\n",
    "    if plot:\n",
    "        plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, c = labels, cmap = matplotlib.colors.ListedColormap(colors))\n",
    "        red_patch = mpatches.Patch(color='orange', label='Negative')\n",
    "        green_patch = mpatches.Patch(color='blue', label='Positive')\n",
    "        plt.legend(handles=[red_patch, green_patch], prop={'size': 25})\n",
    "        \n",
    "fig = plt.figure(figsize =(10,10))\n",
    "plot_lsa(X_train_tfidf, y_train)\n",
    "plt.xlim(0,0.5)\n",
    "plt.ylim(0,0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Building and Evaluation\n",
    "\n",
    "## 2.1 Model Building\n",
    "Here we will compare a simple logistic regression with a SVM that is tweaked with GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "logr = LogisticRegressionCV()\n",
    "logr.fit(X_train_tfidf, y_train)\n",
    "y_pred_logr = logr.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "clf = SVC(class_weight = 'balanced')\n",
    "pipe = Pipeline([('classifier', clf)])\n",
    "fit_params = {'classifier__kernel':['rbf', 'linear', 'poly'],\n",
    "          'classifier__degree':[2, 3, 4],\n",
    "          'classifier__C':[0.01, 0.1, 1, 10]}\n",
    "\n",
    "gs = GridSearchCV(pipe, fit_params, cv = 10, return_train_score = True)\n",
    "gs.fit(X_train_tfidf, y_train)\n",
    "\n",
    "\n",
    "print('Best performing classifier parameters (score {}):\\n{}'.format(gs.best_score_, gs.best_params_))\n",
    "\n",
    "pipe.set_params(classifier__degree = gs.best_params_['classifier__degree'],\n",
    "                classifier__kernel = gs.best_params_['classifier__kernel'],\n",
    "               classifier__C = gs.best_params_['classifier__C'])\n",
    "pipe.fit(X_train_tfidf, y_train)\n",
    "y_pred = pipe.predict(X_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the best performing SVM had a linear kernel and default regularization strength. This might mean that in practice the best possible SVM and a simple Logistic Regression might be about the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Model Evaluation\n",
    "\n",
    "Below we see that the Logistic Regression has a slight performance lead on the linear SVM. At 87.5% accuracy, we have built a model that is pretty good given the limited data we had. \n",
    "\n",
    "Guessing the most comon class would give 74.2% accurace, so our classifier is doing well. Note a fairly even precision and recall for both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#Logistic Regression Eval\n",
    "print('Logistic Regression Accuracy: ', accuracy_score(y_test, y_pred_logr))\n",
    "print('\\nLogistic Classification Report: \\n' , classification_report(y_test,  y_pred_logr))\n",
    "\n",
    "#SVM Eval\n",
    "print('\\n\\n SVM Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('\\nSVM Classification Report: \\n' , classification_report(y_test,  y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Confusion Matrix\n",
    "Our model has a preference towards giving false negatives rather than false positives. We look at some examples of misclassified tweets below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.winter):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=30)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize=20)\n",
    "    plt.yticks(tick_marks, classes, fontsize=20)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", \n",
    "                 color=\"white\" if cm[i, j] < thresh else \"black\", fontsize=40)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=30)\n",
    "    plt.xlabel('Predicted label', fontsize=30)\n",
    "    return plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_logr)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plot = plot_confusion_matrix(cm, classes=['Negative','Positive'], normalize=True, title='Confusion matrix')\n",
    "plt.show()\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples of False Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fn(ypred, ytrue):\n",
    "    y_fn  = X_test[(ypred==0)&(ypred!=ytrue)]\n",
    "    return y_fn\n",
    "\n",
    "fn = find_fn(y_pred_logr, y_test)\n",
    "print('Number of False Negatives: ', fn.shape, '\\n')\n",
    "print(fn[:1].values, '\\n')\n",
    "print(fn[1:2].values, '\\n')\n",
    "print(fn[2:3].values, '\\n')\n",
    "print(fn[3:4].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples of False Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fp(ypred, ytrue):\n",
    "    y_fp  = X_test[(ypred==1)&(ypred!=ytrue)]\n",
    "    return y_fp\n",
    "\n",
    "fp = find_fp(y_pred_logr, y_test)\n",
    "print('Number of False Positives: ', fp.shape, '\\n')\n",
    "print(fp[:1].values, '\\n')\n",
    "print(fp[1:2].values, '\\n')\n",
    "print(fp[2:3].values, '\\n')\n",
    "print(fp[3:4].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Analyzing Model Error\n",
    "It seems that some tweets are not obviously positive or negative. Recall that our original DataFrame had a column labeled \"sentiment:confidence\". We can use this to distinguish Tweets that are vague or not clear to a human eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_wrong(ypred, ytrue):\n",
    "    y_wrong  = X_test[(ypred!=ytrue)]\n",
    "    low_conf = y_wrong[X_test['sentiment:confidence']<0.3]\n",
    "    return y_wrong, low_conf\n",
    "\n",
    "y_wrong, low_conf = find_wrong(y_pred_logr, y_test)\n",
    "plt.hist(y_wrong.shape, label = 'Confidence >= 0.30')\n",
    "plt.hist(low_conf.shape, label = 'Confidence < 0.30')\n",
    "plt.title(\"Human Confidence of Mislabelled Data\")\n",
    "plt.xticks([])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['model:confidence'] = logr.predict_proba(X_test_tfidf)\n",
    "ones = X_test[X_test['sentiment']==1]\n",
    "zeroes = X_test[X_test['sentiment']==0]\n",
    "ones = ones.sort_values(by = ['sentiment:confidence'])\n",
    "zeroes = zeroes.sort_values(by = ['sentiment:confidence'], ascending = False)\n",
    "\n",
    "human_conf = zeroes['sentiment:confidence'].values + ones['sentiment:confidence'].values\n",
    "model_conf = zeroes['model:confidence'].values + ones['model:confidence'].values\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.bar(list(range(human_conf.shape[0])), human_conf, color = 'green', label = 'Human Confidence')\n",
    "plt.bar(list(range(model_conf.shape[0])), model_conf, color = 'orange', label = 'Model Confidence')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Word Importances\n",
    "As illustrated by the image below, our model clearly picks up on words that might predict s positive or negative tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_important_features(vectorizer, model, n=5):\n",
    "    index_to_word = {v:k for k,v in vectorizer.vocabulary_.items()}\n",
    "    \n",
    "    # loop for each class\n",
    "    classes ={}\n",
    "    for class_index in range(model.coef_.shape[0]):\n",
    "        word_importances = [(el, index_to_word[i]) for i,el in enumerate(model.coef_[class_index])]\n",
    "        sorted_coeff = sorted(word_importances, key = lambda x : x[0], reverse=True)\n",
    "        tops = sorted(sorted_coeff[:n], key = lambda x : x[0])\n",
    "        bottom = sorted_coeff[-n:]\n",
    "        classes[class_index] = {\n",
    "            'tops':tops,\n",
    "            'bottom':bottom\n",
    "        }\n",
    "    return classes\n",
    "\n",
    "importance = get_most_important_features(vect, logr, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_important_words(top_scores, top_words, bottom_scores, bottom_words, name):\n",
    "    y_pos = np.arange(len(top_words))\n",
    "    top_pairs = [(a,b) for a,b in zip(top_words, top_scores)]\n",
    "    top_pairs = sorted(top_pairs, key=lambda x: x[1])\n",
    "    \n",
    "    bottom_pairs = [(a,b) for a,b in zip(bottom_words, bottom_scores)]\n",
    "    bottom_pairs = sorted(bottom_pairs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    top_words = [a[0] for a in top_pairs]\n",
    "    top_scores = [a[1] for a in top_pairs]\n",
    "    \n",
    "    bottom_words = [a[0] for a in bottom_pairs]\n",
    "    bottom_scores = [a[1] for a in bottom_pairs]\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 10))  \n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.barh(y_pos,bottom_scores, align='center', alpha=0.5)\n",
    "    plt.title('Negative Sentiment', fontsize=20)\n",
    "    plt.yticks(y_pos, bottom_words, fontsize=14)\n",
    "    plt.suptitle('Key words', fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=20)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.barh(y_pos,top_scores, align='center', alpha=0.5)\n",
    "    plt.title('Positive Sentiment', fontsize=20)\n",
    "    plt.yticks(y_pos, top_words, fontsize=14)\n",
    "    plt.suptitle(name, fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=20)\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.8)\n",
    "    plt.show()\n",
    "\n",
    "top_scores = [a[0] for a in importance[0]['tops']]\n",
    "top_words = [a[1] for a in importance[0]['tops']]\n",
    "bottom_scores = [a[0] for a in importance[0]['bottom']]\n",
    "bottom_words = [a[1] for a in importance[0]['bottom']]\n",
    "\n",
    "plot_important_words(top_scores, top_words, bottom_scores, bottom_words, \"Most Important Words for Sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Conclusion\n",
    "\n",
    "We built a linear model that predicts the sentiment of tweets about Apple at around 87.5% accuracy.  The Confusion Matrix showed a tendency towards false negatives. Lastly we showed that the model succesfully inferred the importance of some english words to twitter sentiment."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#999; background:#fff;\">\n",
    "Created with Jupyter written by Harrison Jansma.\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
