{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightning Tour\n",
    "\n",
    "## Get Tokens, Noun Chunks, and Sentences\n",
    "\n",
    "- .text\n",
    "- .noun_chunks\n",
    "- .sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huckleberry is the greatest dog in the world. He runs fast, is super chill, and smells like cinnamon. \n",
      "\n",
      "Huckleberry\n",
      "runs fast, is \n",
      "\n",
      "[Huckleberry, the greatest dog, the world, He, cinnamon] \n",
      "\n",
      "He runs fast, is super chill, and smells like cinnamon.\n",
      "[He, cinnamon]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(\"Huckleberry is the greatest dog in the world. He runs fast, is super chill, and smells like cinnamon.\")\n",
    "print(doc.text, '\\n')\n",
    "\n",
    "#tokens with the .text attribute\n",
    "print(doc[0].text)\n",
    "print(doc[10:14].text, '\\n')\n",
    "\n",
    "#noun chunks with .noun_chunks attribute\n",
    "noun_chunks = list(doc.noun_chunks)\n",
    "print(noun_chunks, '\\n')\n",
    "\n",
    "#sentence with .sents attribute\n",
    "sentences = list(doc.sents)\n",
    "print(sentences[1])\n",
    "\n",
    "\n",
    "#sents + noun chunks\n",
    "print(list(sentences[1].noun_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of Speech Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sandalwood strapped to the ceiling. \n",
      "\n",
      "Word:  sandalwood\n",
      "POS Tag:  NOUN 91\n",
      "Word Shape:  xxxx 13110060611322374290\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"The sandalwood strapped to the ceiling.\")\n",
    "print(doc.text, '\\n')\n",
    "\n",
    "\n",
    "print (\"Word: \", doc[1].text)\n",
    "print(\"POS Tag: \", doc[1].pos_, doc[1].pos)\n",
    "print(\"Word Shape: \", doc[1].shape_, doc[1].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognize Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "San Francisco 0 13 GPE\n",
      "FB 0 2 ORG\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'San Francisco considers banning sidewalk delivery robots')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "from spacy.tokens import Span\n",
    "doc = nlp(u'FB is hiring a new VP of global policy')\n",
    "doc.ents = [Span(doc, 0, 1, label=doc.vocab.strings[u'ORG'])]\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize a Dependency Parse"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(\"I threw the ball to Huck.\")\n",
    "displacy.serve(doc, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors and Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wood : banana  0.2815443\n",
      "wood:  True \n",
      " banana:  True\n"
     ]
    }
   ],
   "source": [
    "nlp2 = spacy.load('en_core_web_md')\n",
    "doc = nlp2(\"Wood and banana are the fuel of life.\")\n",
    "\n",
    "wood = doc[0]\n",
    "banana = doc[2]\n",
    "fuel = doc[5]\n",
    "life = doc[7]\n",
    "\n",
    "print('wood : banana ', wood.similarity(banana))\n",
    "print ('wood: ', wood.has_vector,'\\n banana: ', banana.has_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization (Saving)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huckleberry is the best dog in the world\n"
     ]
    }
   ],
   "source": [
    "doc = nlp2(\"Huckleberry is the best dog in the world\")\n",
    "print(doc.text)\n",
    "\n",
    "#Need to get this to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match Text with Token Rules\n",
    "???\n",
    "Need to figure out fuzzy matching ASAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman Woman\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher  =  Matcher(nlp.vocab)\n",
    "pattern = [{'ORTH':'Woman'}]\n",
    "matcher.add(\"woman\", None, pattern)\n",
    "\n",
    "doc= nlp('woman, Woman, women')\n",
    "matches = matcher(doc)\n",
    "\n",
    "for idx, strt, end in matches:\n",
    "    string = nlp.vocab.strings[idx]\n",
    "    span = doc[strt:end]\n",
    "    print(string, span)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
