{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spacy\n",
    "\n",
    "## Everything you Need to Know about Spacy\n",
    "\n",
    "https://spacy.io/usage/spacy-101\n",
    "\n",
    "\n",
    "Spacy uses neural network models, trained on classical NLP datasets, to predict the NLP data of a sentence. There are different model that vary for different use cases. Some are larger and more accurate, some trained on different kinds of data, some predict different things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Features\n",
    "\n",
    "- Tokenization -- Segmenting your text. \n",
    "- Parts- of Speech Tagging -- Assigning grammatical word types to individual words in a sentence.\n",
    "- Dependency Parsing -- Assigning dependency labels that describe relationships between tokens.\n",
    "- Lemmatization -- Assigning the base form of a word\n",
    "- Sentenc Boundary Detection -- Finding and Segmenting individual sentences.\n",
    "- Named Entity Recoginition -- Label real world objects.\n",
    "- Similarity -- comparing two textual documents to determine similarity.\n",
    "- Text Classification -- Assigning categories and labels to a document or subdocument.\n",
    "- Rule Based Matching -- regex\n",
    "- Training -- Statistical model predictions?\n",
    "- Serialization -- Saving objects to files or bite strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English Models\n",
    "Downloadable statistical models for spaCy to predict and assign features. Most are CNNs with residual connections, layer normalization, and maxout nonlinearity.\n",
    "\n",
    "- tagging\n",
    "- parsing\n",
    "- entity recognition\n",
    "\n",
    "### en_core_web_sm\n",
    "English multitask CNN assigns content specific token vectors, Parts of Speech tags, dependency parsing, and Named Entity extraction. 29MB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Harrison and I do not likely Apple Music. \n",
      "\n",
      "My ADJ poss\n",
      "name NOUN nsubj\n",
      "is VERB ROOT\n",
      "Harrison PROPN attr\n",
      "and CCONJ cc\n",
      "I PRON nsubj\n",
      "do VERB aux\n",
      "not ADV neg\n",
      "likely ADV conj\n",
      "Apple PROPN compound\n",
      "Music PROPN dobj\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# load the model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#assign avariable with the models output.\n",
    "doc = nlp(\"My name is Harrison and I do not likely Apple Music.\")\n",
    "print(doc.text, '\\n')\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linguistic Annotation \n",
    "\n",
    "Load a model with spacy.load(). Which returns a language model that is referred to as nlp. Call nlp on a doc to return a compressed doc, containing the word type, POS, and dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sometimes ADV advmod\n",
      "I PRON nsubj\n",
      "cry VERB ROOT\n",
      "myself PRON dobj\n",
      "to PART aux\n",
      "sleep VERB ccomp\n",
      "at ADP prep\n",
      "night NOUN pobj\n",
      "thinking VERB advcl\n",
      "about ADP prep\n",
      "Donald PROPN compound\n",
      "Trump PROPN pobj\n",
      "and CCONJ cc\n",
      "Brexit PROPN conj\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Sometimes I cry myself to sleep at night thinking about Donald Trump and Brexit.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Each document is tokenized by rules specific to each language.\n",
    "Raw text is split on whitespace, then the tokenizer iterates over the text.\n",
    "\n",
    "Checks:\n",
    "1. Does the substring match a tokenizer exception?\n",
    "2. Can a prefix, suffix, or infix be split off?\n",
    "\n",
    "        Prefix: Character(s) at the beginning, e.g. $, (, “, ¿.\n",
    "\n",
    "        Suffix: Character(s) at the end, e.g. km, ), ”, !.\n",
    "\n",
    "        Infix: Character(s) in between, e.g. -, --, /, ….\n",
    "\n",
    "If the substring matches to an above exception, the substring is modified and the tokenizer continues its iteration through the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chimpanzees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sunshine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Text\n",
       "0  Chimpanzees\n",
       "1        drink\n",
       "2         boba\n",
       "3            -\n",
       "4          tea\n",
       "5           in\n",
       "6          the\n",
       "7     sunshine\n",
       "8            ."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "doc = nlp(\"Chimpanzees drink boba-tea in the sunshine.\")\n",
    "df = pd.DataFrame([token.text for token in doc], columns = [\"Text\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts-of-Speech Tags and Dependencies\n",
    "\n",
    "Once tokenization is complete, we begin parsing and tagging the doc. The statistical model makes a prediction about what tag is most likely to be appropriate in this context.\n",
    "\n",
    "\n",
    "Linguistic annotations are available as Token attributes. Spacy encodes all strings to hash values (one way conversion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tbext</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>dep</th>\n",
       "      <th>shape</th>\n",
       "      <th>isalpha</th>\n",
       "      <th>isstop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R</td>\n",
       "      <td>r</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>X</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is</td>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>det</td>\n",
       "      <td>x</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>strange</td>\n",
       "      <td>strange</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>amod</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coding</td>\n",
       "      <td>coding</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>compound</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>language</td>\n",
       "      <td>language</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>attr</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>punct</td>\n",
       "      <td>.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Strange</td>\n",
       "      <td>strange</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>yet</td>\n",
       "      <td>yet</td>\n",
       "      <td>ADV</td>\n",
       "      <td>RB</td>\n",
       "      <td>cc</td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>speedy</td>\n",
       "      <td>speedy</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>conj</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>punct</td>\n",
       "      <td>.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tbext     lemma    pos  tag       dep  shape  isalpha  isstop\n",
       "0          R         r   NOUN   NN     nsubj      X     True   False\n",
       "1         is        be   VERB  VBZ      ROOT     xx     True    True\n",
       "2          a         a    DET   DT       det      x     True    True\n",
       "3    strange   strange    ADJ   JJ      amod   xxxx     True   False\n",
       "4     coding    coding   NOUN   NN  compound   xxxx     True   False\n",
       "5   language  language   NOUN   NN      attr   xxxx     True   False\n",
       "6          .         .  PUNCT    .     punct      .    False   False\n",
       "7    Strange   strange  PROPN  NNP      ROOT  Xxxxx     True   False\n",
       "8        yet       yet    ADV   RB        cc    xxx     True    True\n",
       "9     speedy    speedy    ADJ   JJ      conj   xxxx     True   False\n",
       "10         .         .  PUNCT    .     punct      .    False   False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def df_build(text):\n",
    "    doc  = nlp(text)\n",
    "    outr = []\n",
    "    for token in doc:\n",
    "        inr = [token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop]\n",
    "        outr.append(inr)\n",
    "    df = pd.DataFrame(outr, columns = [\"tbext\", \"lemma\", \"pos\", \"tag\", \"dep\", \"shape\", \"isalpha\", \"isstop\"])    \n",
    "    return df\n",
    "    \n",
    "df = df_build(\"R is a strange coding language. Strange yet speedy.\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun, singular or mass\n",
      "verb, 3rd person singular present\n",
      "adjective\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain(\"NN\"))\n",
    "print(spacy.explain(\"VBZ\"))\n",
    "print(spacy.explain(\"JJ\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entities\n",
    "\n",
    "Available in the ents attribute of a Doc container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "McDonald's 37 47 ORG\n",
      "US 61 63 GPE\n",
      "Harrison 65 73 PERSON\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Chicken nuggets are the best part of McDonald's found in the US. Harrison eats them everyday.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors and Similarity\n",
    "\n",
    "Compare 2 projects and make a prediction on their similarity to each other. Useful for many things. Each Doc, Span, Token comes with a .similarity() method thar allows for comparison with another object.\n",
    "\n",
    "Word embeddings are generated with word2Vec. Small models do not come with word embeddings.\n",
    "\n",
    "Doc vectors will average the vectors of its tokens.\n",
    "\n",
    "- Text: The original token text.\n",
    "- has vector: Does the token have a vector representation?\n",
    "- Vector norm: The L2 norm of the token's vector (the square root of the sum - of the values squared)\n",
    "- OOV: Out-of-vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chicken chicken 1.0\n",
      "chicken banana 0.50540304\n",
      "chicken spoon 0.46489623\n",
      "banana chicken 0.50540304\n",
      "banana banana 1.0\n",
      "banana spoon 0.44800603\n",
      "spoon chicken 0.46489623\n",
      "spoon banana 0.44800603\n",
      "spoon spoon 1.0\n"
     ]
    }
   ],
   "source": [
    "nlp2 = spacy.load('en_core_web_md')  # make sure to use larger model!\n",
    "doc = nlp2('chicken banana spoon')\n",
    "for token1 in doc:\n",
    "    for token2 in doc:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "(300,)\n",
      "(300,)\n",
      "Doc Vector: \n",
      "[-0.17136002 -0.18593933  0.42900333]  ...  [-0.69952327  0.03642799  0.31438032]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp2('chicken banana spoon')\n",
    "for token1 in doc:\n",
    "    #vectors are long, just showing shape\n",
    "    print(token1.vector.shape)\n",
    "print(\"Doc Vector: \")\n",
    "print(doc.vector[:3], \" ... \", doc.vector[-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "Start out by tokenizing the text document, then pass down a pipeline of POS tagger, dependency parser, entity recognizer,... Each is a seperate statistical model that predicts objects for each token.\n",
    "\n",
    "You can mess with the pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab, Hashes, and Lexemes\n",
    "\n",
    "Spacy stores data in a library to save space in RAM. This vocab is shared by many document items. String values are encoded to hash values.\n",
    "\n",
    "- Token: A word, punctuation mark etc. in context, including its attributes, tags and dependencies.\n",
    "- Lexeme: A \"word type\" with no context. Includes the word shape and flags, e.g. if it's lowercase, a digit or punctuation.\n",
    "- Doc: A processed container of tokens in context.\n",
    "- Vocab: The collection of lexemes.\n",
    "- StringStore: The dictionary mapping hash values to strings, for example 3197928453018144401 → \"coffee\".\n",
    "\n",
    "One word used many times in multiple documents is only stored once. StringStore is the conversion between string and hash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3197928453018144401\n",
      "coffee\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'I love coffee')\n",
    "print(doc.vocab.strings[u'coffee'])  # 3197928453018144401\n",
    "print(doc.vocab.strings[3197928453018144401])  # 'coffee'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 4690420944186131903 X I I True False True en\n",
      "love 3702023516439754181 xxxx l ove True False False en\n",
      "coffee 3197928453018144401 xxxx c fee True False False en\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'I love coffee')\n",
    "for word in doc:\n",
    "    lexeme = doc.vocab[word.text]\n",
    "    print(lexeme.text, lexeme.orth, lexeme.shape_, lexeme.prefix_, lexeme.suffix_,\n",
    "          lexeme.is_alpha, lexeme.is_digit, lexeme.is_title, lexeme.lang_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Text: The original text of the lexeme.\n",
    "- Orth: The hash value of the lexeme.\n",
    "- Shape: The abstract word shape of the lexeme.\n",
    "- Prefix: By default, the first letter of the word string.\n",
    "- Suffix: By default, the last three letters of the word string.\n",
    "- is alpha: Does the lexeme consist of alphabetic characters?\n",
    "- is digit: Does the lexeme consist of digits?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
