{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping\n",
    "\"Bootstrapping is a statistical method for estimating the sampling distribution of an estimator by sampling with replacement from the original sample,\"\n",
    "\n",
    "https://en.wikipedia.org/wiki/Resampling_(statistics)\n",
    "\n",
    "\n",
    "Essentially used to make a robust estimate of any true population measure... mean, proportion, odds. etc\n",
    "\n",
    "Random subselection of your sample will drive down variance and reduce the effect on potential outliers or weird coincidences/ effects in your data.\n",
    "\n",
    "Anything that you might calculate with data from a sample can be effected by bootstrapping. May or may not improve your estimate\n",
    "\n",
    "The bootstrap estimates the variability of your statistic based on the variability from subsamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "Model validation technique asses how the model  will generalize. Goal of cross validation define a data set to test on within the training period. \"validation set\"\n",
    "\n",
    "\n",
    "Partition you known data into subsets. Train on one test on the other. Repeat the process multiple times with random partitions to reduce variability of your estimates. average the validation (test) results of each round.\n",
    "\n",
    "\n",
    "## K Fold\n",
    "\n",
    "In stratified k-fold cross-validation, the folds are selected so that the mean response value is approximately equal in all the folds. In the case of a dichotomous classification, this means that each fold contains roughly the same proportions of the two types of class labels.\n",
    "\n",
    "\n",
    "Is a way to not have to rely on training accuracy, which is inherently overly optimistic of a classifiers prediction performance due to some unknown amount of overfitting. We seek to replicate the preocess of train test split on our training data, so that we can 1. compare algorithms, 2. select features.\n",
    "\n",
    "Cross Validation is an estimation of the expected fit of our model to training data, as such it has some variability. Means it will be less optimistic about model prediction performance. When true testing time comes, the model will perform slightly better than CV mean outcome\n",
    "\n",
    "\n",
    "Cross Validation only yields meaningful results if the validation and training set are drawn from the same population and only if human biases are controlled. A model will not be able to generalize effectively to something that is in flux constantly. (ie stock markets) Changes over time might make the set the model was trained on significantly different from the observations you are trying to predict. Make sure your training data always represents the population you are trying to generalize to.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Correctly use Cross Validation in Predictive Modelling\n",
    "\n",
    "https://stuartlacy.co.uk/2016/02/04/how-to-correctly-use-cross-validation-in-predictive-modelling/\n",
    "\n",
    "Circle back to this tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation Techniques\n",
    "\n",
    "https://stats.stackexchange.com/questions/103459/how-do-i-know-which-method-of-cross-validation-is-best\n",
    "\n",
    "There are many cross validation techniques:\n",
    "\n",
    "b) hold-out is the the method #1 above. Split the set into a training and one test. There is a long history of discussion and practices on the relative sizes of the training and test set.\n",
    "\n",
    "c) k-fold – method #2 above. Pretty standard.\n",
    "\n",
    "d) Leave-one-out – method #3 above.\n",
    "\n",
    "e) bootstrap: if your set has N data, randomly select N samples WITH REPLACEMENT from the set and use it as training. The data from the original set that has not been samples any time is used as the test set. There are different ways to compute the final estimation of the error of the model which uses both the error for the test set (out-of-sample) and the error for the train set (in-sample). See for example, the .632 bootstrap. I think there is also a .632+ formula – they are formulas that estimate the true error of the model using both out-of-sample and in-sample errors.\n",
    "\n",
    "\n",
    "Is a method biased for the estimation of the model error.\n",
    "\n",
    "How fast does the method converge to the true model error.\n",
    "\n",
    "Given a prediction of model error how certain can you be that the prediction is close to the true value?\n",
    "\n",
    "\n",
    "\n",
    "In general, in this case you want to take many measures of the error and calculate a confidence interval (or a credible interval if you follow a Bayesian approach). In this case, the issue is how much can you trust the variance of the set of error measures. Notice that except for the leave-one-out, all techniques above will give you many different measures (k measures for a k-fold, n measures for a n-repeated hold out) and thus you can measure the variance (or standard deviation) of this set and calculate a confidence interval for the measure of error.\n",
    "\n",
    "\n",
    "Ant trust variance from Kfold with large k, you can trust the mean since the mean regresses to the true average. With repeat K fold (doing k fold multiple times) you can super trust the mean, but the variance will be overly optimistic. The aggregation of many resampling tightens the generalization error prectors variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation or Bootstrapping?\n",
    "\n",
    "https://stats.stackexchange.com/questions/71184/cross-validation-or-bootstrapping-to-evaluate-classification-performance\n",
    "\n",
    "\n",
    "Cross val is usually applied once while bootstrap is repeated many times. In this way K fold has a higher variance on model error estimate.\n",
    "Repeat K fold is the best way.\n",
    "\n",
    "\n",
    "Bootstrapping = bias (pessimistic)\n",
    "\n",
    "K Fold = variance (small k = more bias)\n",
    "\n",
    ". I still would stick by repeated CV for small and large sample sizes.\n",
    "\n",
    "### Choice of Metric:\n",
    "accuracy is not a great scoring metric, because it introduces a  lot of instability ( a prediction is either completely right or completely wrong) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance Estimates in K fold\n",
    "https://stats.stackexchange.com/questions/31190/variance-estimates-in-k-fold-cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Decide K\n",
    "\n",
    "https://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation\n",
    "\n",
    "https://stats.stackexchange.com/questions/9053/how-does-cross-validation-overcome-the-overfitting-problem\n",
    "\n",
    "\n",
    "first of all, in order to lower the variance of the CV result, you can and should repeat/iterate the CV with new random splits. \n",
    "\n",
    "@ogrisel already explained that usually large k mean less (pessimistic) bias. (Some exceptions are known particularly for k=n, i.e. leave-one-out).\n",
    "\n",
    "\n",
    "Too large k mean that only a low number of sample combinations is possible, thus limiting the number of iterations that are different"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
